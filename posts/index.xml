<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on My New Hugo Site</title>
    <link>/posts/</link>
    <description>Recent content in Posts on My New Hugo Site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>/posts/nltk.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/posts/nltk.md/</guid>
      <description> NLTK 1. tokenize https://www.nltk.org/_modules/nltk/tokenize.html
Support sentence, word, for 17 languages source code for sentence tokenizer: ``python def sent_tokenize(text, language=&#39;english&#39;): &amp;quot;&amp;quot;&amp;quot; Return a sentence-tokenized copy of *text*, using NLTK&#39;s recommended sentence tokenizer (currently :class:.PunktSentenceTokenizer` for the specified language).
:param text: text to split into sentences :param language: the model name in the Punkt corpus &amp;quot;&amp;quot;&amp;quot; tokenizer = load(&#39;tokenizers/punkt/{0}.pickle&#39;.format(language)) return tokenizer.tokenize(text)  word tokenizer: word_tokenize(text, language=&amp;lsquo;english&amp;rsquo;, preserve_line=False) perserve_line == false, then call sentence tokenizer first, otherwise, don&amp;rsquo;t </description>
    </item>
    
    <item>
      <title></title>
      <link>/posts/pandascheatsheet.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/posts/pandascheatsheet.md/</guid>
      <description> Pandas 1. Collect dataframe as dictionary .set_index([&amp;lsquo;a&amp;rsquo;,&amp;lsquo;b&amp;rsquo;]).T.to_dict(&amp;lsquo;list&amp;rsquo;)
2. Read in csv file format(transpose) https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html
3. count rows with same value df[col].value_counts()
4. display columns unlimit ( equivalent to spark df, with limit=False) pd.set_option(&#39;display.expand_frame_repr&#39;, False) Other settings: pd.set_option(&#39;display.height&#39;, 1000) pd.set_option(&#39;display.max_rows&#39;, 500) pd.set_option(&#39;display.max_columns&#39;, 500) pd.set_option(&#39;display.width&#39;, 1000)  5. remove all the rows with a value occur less than n times df[df.groupby(value).uid.transform(len) &amp;gt; n] or: df.groupby(by=value).filter(lambda x: len(x) &amp;gt; n)  </description>
    </item>
    
    <item>
      <title></title>
      <link>/posts/pysparkcheatsheet.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/posts/pysparkcheatsheet.md/</guid>
      <description>Pyspark 1. initilaztion 1. spark session from pyspark.sql import SparkSession spark = SparkSession \ .builder \ .appName(&amp;quot;name&amp;quot;) \ .config(&amp;quot;spark.some.config.option&amp;quot;, &amp;quot;some-value&amp;quot;) \ .getOrCreate()  2. spark context 1. from spark session sc = spark.sparkContext  2. from spark context conf = SparkConf().setAppName(&amp;quot;KMeans&amp;quot;).setMaster(&amp;quot;local[*]&amp;quot;) sc = SparkContext(conf =conf) or sc = SparkContext.getOrCreate(conf)  2. paritition by index .mapPartitionsWithIndex(lambda idx, it: islice(it, 1, None) if idx == 0 else it)\ this get rid of the first line(rdd) in the file.</description>
    </item>
    
    <item>
      <title></title>
      <link>/posts/pythoncheatsheet.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/posts/pythoncheatsheet.md/</guid>
      <description>Generic Python 1. Open file, read, write Open: f = open(“hello.text”, flag), flag: &#39;r&#39; = read, &#39;b&#39; = binary, &#39;w&#39; = write read sing line: f.readline() read a list of lines: f.readlines() read whole file as string: f.read()  2. Read from stdin import sys for line in sys.stdin.readlines(): #convert each line from a string to a list of integer line_list = (map(int, line.split()))  2. look up table of element, origin index -&amp;gt; the sorted index idx = np.</description>
    </item>
    
    <item>
      <title></title>
      <link>/posts/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/posts/readme/</guid>
      <description>PythonNotes Cheat notes for Pyspark, Pandas, Numpy, NLTK, Keras, Tensorflow and etc</description>
    </item>
    
    <item>
      <title></title>
      <link>/posts/stackeditcheatsheet.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/posts/stackeditcheatsheet.md/</guid>
      <description>Welcome to StackEdit! Hi! I&amp;rsquo;m your first Markdown file in StackEdit. If you want to learn about StackEdit, you can read me. If you want to play with Markdown, you can edit me. Once you have finished with me, you can create new files by opening the file explorer on the left corner of the navigation bar.
Files StackEdit stores your files in your browser, which means all your files are automatically saved locally and are accessible offline!</description>
    </item>
    
    <item>
      <title></title>
      <link>/posts/toolkit.md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/posts/toolkit.md/</guid>
      <description>Other useful packages 1. track memory usage line by line https://pypi.org/project/memory_profiler/
2. track run time for each line https://pypi.org/project/line_profiler/
3. regular expression https://docs.python.org/3/library/re.html
4. google word to vector, trained model GoogleNews-vectors-negative300.bin https://code.google.com/archive/p/word2vec/</description>
    </item>
    
  </channel>
</rss>