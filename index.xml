<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hello World, This Is J</title>
    <link>/</link>
    <description>Recent content on Hello World, This Is J</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Apr 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Difference between Multi-Processing, Multi-threading and Coroutine</title>
      <link>/post/tcp/</link>
      <pubDate>Tue, 09 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tcp/</guid>
      <description>How is a computer program executed? A program needs at least one thread to run on. And a coroutine live in a thread, a thread lives in process, a process lives in core, a core lives in a CPU.
 Multi-Processing usually refer to many processes execute in parallel. Process is smallest resource management unit, different process share different resource. Multi-Threading usually refer to many threads execute concurrently, when there are idle cores, threads can use idle cores to run in parallel.</description>
    </item>
    
    <item>
      <title>Sublime ToolKit</title>
      <link>/post/sublimetoolkit/</link>
      <pubDate>Tue, 09 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/sublimetoolkit/</guid>
      <description>Useful sublime packages, steps to install: 1. open sublime 2. open command palette (Ctrl+Shift+P for Windows/Linux, Cmd+Shift+P for Mac OS) 3. type installSearch for Package Control: Install Package and hit Enter. 4 type &amp;quot;package_name&amp;quot;, find the package and hit enter
1.gitgutter very useful tool if you are working with a git repo
2.SumlineLinter &amp;amp; SumlineLinter-flake8 linter 3. KiteSublime to search documentations &amp;gt; Written with StackEdit. </description>
    </item>
    
    <item>
      <title>Python CheatSheet</title>
      <link>/post/pythoncheatsheet/</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/pythoncheatsheet/</guid>
      <description>This cheatsheet includes some very basic but easy to forget operations and some random notes. A good tutorial for beginner in Chinese 1. Decorator Syntax def decorator(func): def new_func(*args, **argkw): #add stuff print(&amp;quot;Hello World&amp;quot;) return func(*args, **argkw) return new_func @decorator def f(args): pass #run function f f() #result: #Hello World  2. Open file, read, write Open: f = open(“hello.text”, flag), flag: &#39;r&#39; = read, &#39;b&#39; = binary, &#39;w&#39; = write read sing line: f.</description>
    </item>
    
    <item>
      <title>Useful tool kit</title>
      <link>/post/pythontoolkit/</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/pythontoolkit/</guid>
      <description>Useful tools could be used in python development
1. track memory usage line by line https://pypi.org/project/memory_profiler/
2. track run time for each line https://pypi.org/project/line_profiler/
3. regular expression https://docs.python.org/3/library/re.html
4. google word to vector, trained model GoogleNews-vectors-negative300.bin https://code.google.com/archive/p/word2vec/
5. trace execution for each line/function python -m trace --count -C . somefile.py ...  https://docs.python.org/3.7/library/trace.html</description>
    </item>
    
    <item>
      <title>NLTK</title>
      <link>/post/nltk/</link>
      <pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/nltk/</guid>
      <description> 1. tokenize https://www.nltk.org/_modules/nltk/tokenize.html
Support sentence, word, for 17 languages source code for sentence tokenizer: ``python def sent_tokenize(text, language=&#39;english&#39;): &amp;quot;&amp;quot;&amp;quot; Return a sentence-tokenized copy of *text*, using NLTK&#39;s recommended sentence tokenizer (currently :class:.PunktSentenceTokenizer` for the specified language).
:param text: text to split into sentences :param language: the model name in the Punkt corpus &amp;quot;&amp;quot;&amp;quot; tokenizer = load(&#39;tokenizers/punkt/{0}.pickle&#39;.format(language)) return tokenizer.tokenize(text)  word tokenizer: word_tokenize(text, language=&amp;lsquo;english&amp;rsquo;, preserve_line=False) perserve_line == false, then call sentence tokenizer first, otherwise, don&amp;rsquo;t </description>
    </item>
    
    <item>
      <title>Pandas Cheat Sheet</title>
      <link>/post/pandascheatsheet/</link>
      <pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/pandascheatsheet/</guid>
      <description> 1. Collect dataframe as dictionary .set_index([&amp;lsquo;a&amp;rsquo;,&amp;lsquo;b&amp;rsquo;]).T.to_dict(&amp;lsquo;list&amp;rsquo;)
2. Read in csv file format(transpose) https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html
3. count rows with same value df[col].value_counts()
4. display columns unlimit ( equivalent to spark df, with limit=False) pd.set_option(&#39;display.expand_frame_repr&#39;, False) Other settings: pd.set_option(&#39;display.height&#39;, 1000) pd.set_option(&#39;display.max_rows&#39;, 500) pd.set_option(&#39;display.max_columns&#39;, 500) pd.set_option(&#39;display.width&#39;, 1000)  5. remove all the rows with a value occur less than n times df[df.groupby(value).uid.transform(len) &amp;gt; n] or: df.groupby(by=value).filter(lambda x: len(x) &amp;gt; n)  </description>
    </item>
    
    <item>
      <title></title>
      <link>/post/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/post/readme/</guid>
      <description>PythonNotes Cheat notes for Pyspark, Pandas, Numpy, NLTK, Keras, Tensorflow and etc</description>
    </item>
    
  </channel>
</rss>