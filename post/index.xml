<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Hello World, This Is J</title>
    <link>/post/</link>
    <description>Recent content in Posts on Hello World, This Is J</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 08 Sep 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Pyspark</title>
      <link>/post/pysparkcheatsheet/</link>
      <pubDate>Sun, 08 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/pysparkcheatsheet/</guid>
      <description>1. initilaztion 1. spark session from pyspark.sql import SparkSession spark = SparkSession \ .builder \ .appName(&amp;quot;name&amp;quot;) \ .config(&amp;quot;spark.some.config.option&amp;quot;, &amp;quot;some-value&amp;quot;) \ .getOrCreate()  2. spark context 1. from spark session sc = spark.sparkContext  2. from spark context conf = SparkConf().setAppName(&amp;quot;KMeans&amp;quot;).setMaster(&amp;quot;local[*]&amp;quot;) sc = SparkContext(conf =conf) or sc = SparkContext.getOrCreate(conf)  2. paritition by index .mapPartitionsWithIndex(lambda idx, it: islice(it, 1, None) if idx == 0 else it)\ this get rid of the first line(rdd) in the file.</description>
    </item>
    
    <item>
      <title>Analysis of a few models</title>
      <link>/post/analysis-of-a-few-models/</link>
      <pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/analysis-of-a-few-models/</guid>
      <description>1. OLS Ordinary Least Squares, the most common estimation method for linear models,
It&amp;rsquo;s good and simple, however, it has lots of assumptions.
1. the regression model is linear in coefficients and the error term
2. random sampling of observations
3. No multi-collinearity (each variable is independent)
4. Error term is independent and identically distributed
5. all independent variables are uncorrelated with the error term
In brexit referendum data, there are 110 variables, there would be too much computation.</description>
    </item>
    
    <item>
      <title>Kubernetes Notes</title>
      <link>/post/k8s-notes/</link>
      <pubDate>Thu, 02 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/k8s-notes/</guid>
      <description>Table of Contents:
* Basic Terms
* Manifests
* [Expose a service]()
* [Helm]()
Basic Terms/objects cluster, context, namespace, node, pods.   cluster: physical cluster of physical machine.
 context: a group of access parameters, a cluster, a namespace, a user.
 namespace: virtual cluster, use to separate resources across different environments like prod, dev, test.
 node: a virtual/physical machine
 pod: a set of containers to orchestrate, minimal object to scale</description>
    </item>
    
    <item>
      <title>Difference between Multi-Processing, Multi-threading and Coroutine</title>
      <link>/post/tcp/</link>
      <pubDate>Tue, 09 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tcp/</guid>
      <description>How is a computer program executed? A program needs at least one thread to run on.
And a coroutine live in a thread, a thread lives in process, a process lives in core, a core lives in a CPU.
 Multi-Processing usually refer to many processes execute in parallel. Process is smallest resource management unit, different process share different resource.
 Multi-Threading usually refer to many threads execute concurrently, when there are idle cores, threads can use idle cores to run in parallel.</description>
    </item>
    
    <item>
      <title>Sublime ToolKit</title>
      <link>/post/sublimetoolkit/</link>
      <pubDate>Tue, 09 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/sublimetoolkit/</guid>
      <description>Useful sublime packages,
steps to install:
1. open sublime 2. open command palette `(Ctrl+Shift+P for Windows/Linux, Cmd+Shift+P for Mac OS)` 3. type `install`Search for _Package Control:_ Install Package and hit Enter. 4 type `&amp;quot;package_name&amp;quot; `, find the package and hit enter  1.gitgutter very useful tool if you are working with a git repo
2.SumlineLinter &amp;amp; SumlineLinter-flake8 linter
3. KiteSublime to search documentations
```
&amp;gt; Written with StackEdit.</description>
    </item>
    
    <item>
      <title>Python CheatSheet</title>
      <link>/post/pythoncheatsheet/</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/pythoncheatsheet/</guid>
      <description>This cheatsheet includes some very basic but easy to forget operations and some random notes. A good tutorial for beginner in Chinese 1. Decorator Syntax def decorator(func): def new_func(*args, **argkw): #add stuff print(&amp;quot;Hello World&amp;quot;) return func(*args, **argkw) return new_func @decorator def f(args): pass #run function f f() #result: #Hello World  2. Open file, read, write Open: f = open(“hello.text”, flag), flag: &#39;r&#39; = read, &#39;b&#39; = binary, &#39;w&#39; = write read sing line: f.</description>
    </item>
    
    <item>
      <title>Useful tool kit</title>
      <link>/post/pythontoolkit/</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/pythontoolkit/</guid>
      <description>Useful tools could be used in python development
1. track memory usage line by line https://pypi.org/project/memory_profiler/
2. track run time for each line https://pypi.org/project/line_profiler/
3. regular expression https://docs.python.org/3/library/re.html
4. google word to vector, trained model GoogleNews-vectors-negative300.bin
https://code.google.com/archive/p/word2vec/
5. trace execution for each line/function flow python -m trace --count -C . somefile.py ...  https://docs.python.org/3.7/library/trace.html
6. get time analysis for a statement execution usage: python -m timeit &#39;stmt&#39; # number = 10000 or: timeit.</description>
    </item>
    
    <item>
      <title>CPP Notes</title>
      <link>/post/cpp-notes/</link>
      <pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/cpp-notes/</guid>
      <description>1. lvalue and rvalue you can get memory address for lvalue, you can&amp;rsquo;t for rvalue
2. lvalue reference Reference is cpp is like pointer in c, but not exactly same.
Pass lvalue as parameter, you can just refer to its address, we know its memory address, we can access the data without deep copy, save both time and space.
3. rvalue reference Pass rvalue as parameter, you can&amp;rsquo;t refer to its address(for example, when a function returns, a heap object allocated, but the variable holds the object&amp;rsquo;s heap address will be poped with the stackframe).</description>
    </item>
    
    <item>
      <title>ML-DL Notes</title>
      <link>/post/md-dl-notes/</link>
      <pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/md-dl-notes/</guid>
      <description>Notes for machine learning and deep learning models, design and training techniques
Mainly includes: CNN, RNN(LTSM)
ensemble techniques
activation functions
1. deep learning concept 1. LTSM, a type of RNN, most successful RNNs are LTSM RNNs
1. use a vector as cell state to record state
2. use three gates to erase, update, output the cell state.
Example: Rakuten data challenge
Character-level tokenization
Ensembling:
Bidirectional training
2. Ensemble learning  Bagging(bootstrap aggregating):</description>
    </item>
    
    <item>
      <title>NLTK</title>
      <link>/post/nltk/</link>
      <pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/nltk/</guid>
      <description>1. tokenize https://www.nltk.org/_modules/nltk/tokenize.html
Support sentence, word, for 17 languages
source code for sentence tokenizer:
``python def sent_tokenize(text, language=&#39;english&#39;): &amp;quot;&amp;quot;&amp;quot; Return a sentence-tokenized copy of *text*, using NLTK&#39;s recommended sentence tokenizer (currently :class:.PunktSentenceTokenizer`
for the specified language).
:param text: text to split into sentences :param language: the model name in the Punkt corpus &amp;quot;&amp;quot;&amp;quot; tokenizer = load(&#39;tokenizers/punkt/{0}.pickle&#39;.format(language)) return tokenizer.tokenize(text)  word tokenizer:
word_tokenize(text, language=&amp;lsquo;english&amp;rsquo;, preserve_line=False)
perserve_line == false, then call sentence tokenizer first, otherwise, don&amp;rsquo;t</description>
    </item>
    
    <item>
      <title>Pandas Cheat Sheet</title>
      <link>/post/pandascheatsheet/</link>
      <pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/pandascheatsheet/</guid>
      <description> 1. Collect dataframe as dictionary .set_index([&amp;lsquo;a&amp;rsquo;,&amp;lsquo;b&amp;rsquo;]).T.to_dict(&amp;lsquo;list&amp;rsquo;)
2. Read in csv file format(transpose) https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html
3. count rows with same value df[col].value_counts()
4. display columns unlimit ( equivalent to spark df, with limit=False) pd.set_option(&#39;display.expand_frame_repr&#39;, False) Other settings: pd.set_option(&#39;display.height&#39;, 1000) pd.set_option(&#39;display.max_rows&#39;, 500) pd.set_option(&#39;display.max_columns&#39;, 500) pd.set_option(&#39;display.width&#39;, 1000)  5. remove all the rows with a value occur less than n times df[df.groupby(value).uid.transform(len) &amp;gt; n] or: df.groupby(by=value).filter(lambda x: len(x) &amp;gt; n)  </description>
    </item>
    
    <item>
      <title>Performance</title>
      <link>/post/performance/</link>
      <pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/performance/</guid>
      <description>Table of Contents:
* [Edge Computing]()
* [Content Delivery Network]()
* Server Side
* Gateway: nginx, openResty
* long alive connection
* optimized, cheap, fast
* reverse proxy
* load balancer
* [Application Side]()
* [Algorithms]()
* Disk I/O
* Async
* Disk Performance
* SSD
* RAID 0, 1, 5, 10
* Network I/O
* zip message
* use a better network card &amp;amp; adaptor
 [Database Side]()
 In memory cache -Redis/memcached</description>
    </item>
    
    <item>
      <title>DistributedComputing Notes</title>
      <link>/post/spark-hadoop-notes/</link>
      <pubDate>Thu, 08 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/spark-hadoop-notes/</guid>
      <description>Mostly Hadoop, Spark
As well as concepts, &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- think at scale!
Spark 1. Code Optimization 1. avoid creating duplicate rdds 2. avoid re-calcualtion, reuse same rdd as much as you can,  use cache() to prevent re calculation, (cache rdd in memory)
 use persist() to manually set different level of cache,
 ex.StorageLevel.MEMORY_AND_DISK_SER
 others:
MEMORY_ONLY
MEMORY_AND_DISK
MEMORY_ONLY_SER
MEMORY_AND_DISK_SER: cache to memory first, if memory is not enough, write to disk</description>
    </item>
    
    <item>
      <title>Java Notes</title>
      <link>/post/java-notes/</link>
      <pubDate>Thu, 08 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/java-notes/</guid>
      <description>Java NOtes and cheat sheet I wrote during work and academia
1. ArrayList ArrayList is implemented based on primitive array
Feature:
ArrayList myarrlist = new ArrayList()
1.Type Satety ArrayList provides stronger type safety ensurance than array
https://coderanch.com/t/625190/certification/Array-ArrayList-Thread-safety-Type
2. Flexibility ArrayList has Dynamic, and a better interface
3. Size vs length Size is the capacity
length is the actually length
4. Multi-dimension arraylist doesn&amp;rsquo;t support multi-dimension, array does
5, primitive types ArrayList doesn&amp;rsquo;t support primitive types</description>
    </item>
    
    <item>
      <title>Scala Notes</title>
      <link>/post/scala-notes/</link>
      <pubDate>Thu, 08 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/scala-notes/</guid>
      <description>I wrote more than 5000 lines of Scala code about 1 year ago, but later on I don&amp;rsquo;t have any chance to write it. (University doesn&amp;rsquo;t teach it, previous internships don&amp;rsquo;t use it)
Now I&amp;rsquo;m picking it up as the university teaches spark by Scala, but I feel that I forgot lots of not only syntax but also some key knowledge about Scala.
Therefore I want to write some Scala notes.</description>
    </item>
    
    <item>
      <title></title>
      <link>/post/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/post/readme/</guid>
      <description>PythonNotes Cheat notes for
Pyspark, Pandas, Numpy, NLTK, Keras, Tensorflow and etc</description>
    </item>
    
  </channel>
</rss>